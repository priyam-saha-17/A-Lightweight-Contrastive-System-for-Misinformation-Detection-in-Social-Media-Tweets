{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13825523,"sourceType":"datasetVersion","datasetId":8681919}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:50:22.316848Z","iopub.execute_input":"2025-11-22T11:50:22.317622Z","iopub.status.idle":"2025-11-22T11:50:25.693464Z","shell.execute_reply.started":"2025-11-22T11:50:22.317596Z","shell.execute_reply":"2025-11-22T11:50:25.692355Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# roberta_contrastive_minimal.py\n\"\"\"\nMemory-light contrastive + classifier script based on your original.\nMain changes to reduce memory:\n - roberta-base instead of roberta-large\n - MAX_LEN reduced to 128\n - single encoder forward; two projection views are generated by dropout in proj head\n - encoder frozen by default (only projection + classifier trained)\n - smaller batch size, smaller proj dim\n - gradient clipping\n - optional: unfreeze last N encoder layers (see UNFREEZE_LAST_N)\n\"\"\"\n\nimport os\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, TFRobertaModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# ---------------- CONFIG (tune these) ----------------\nDATA_DIR = \"/kaggle/input/promid-2025/promid_task_3/promid_task_3/\"\nMISINFO_CSV = os.path.join(DATA_DIR, \"misinfo_train.csv\")\nNONMISINFO_CSV = os.path.join(DATA_DIR, \"nonmisinfo_train.csv\")\nTEST_CSV = os.path.join(DATA_DIR, \"test_final_merge_withoutlabel.csv\")\n\nOUTPUT_TRAIN_INTERIM = \"train_final_llm.csv\"\nOUTPUT_TEST_INTERIM = \"test_final_llm.csv\"\nPRED_CSV = \"predictions.csv\"\nPLOTS_DIR = \"plots\"\nMODELS_DIR = \"models\"\n\nPDF_REF_PATH = \"/mnt/data/Fake_News_Detection_in_Social_Media_Hybrid_Deep_Le.pdf\"\n\n# Encoder / training hyperparams (reduced for minimal resources)\nBERT_NAME = \"roberta-base\"   # <-- smaller than large\nMAX_LEN = 512                # <-- much smaller (saves huge memory)\nBATCH_SIZE = 16               # <-- small batch\nEPOCHS = 50                   # fewer epochs (safe default)\nLR = 3e-4                    # heads-only fine-tuning lr\nSEED = 42\n\n# contrastive & heads\nPROJ_DIM = 64\nTEMPERATURE = 0.07\nALPHA = 1.0   # weight for contrastive loss\nBETA = 1.0    # weight for classifier loss\n\n# freeze encoder for memory savings (set False to fine-tune full model)\nFREEZE_ENCODER = True\n# if you want to unfreeze last N encoder layers, set FREEZE_ENCODER=True and UNFREEZE_LAST_N>0\nUNFREEZE_LAST_N = 0\n\n# callbacks\nlr_reduce_patience = 3\nlr_reduce_factor = 0.5\nearly_stop_patience = 6\n\nUSE_MIXED_PRECISION = False  # set False for stability on small setups\n\n# reproducibility\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\nrandom.seed(SEED)\n\n# create dirs\nos.makedirs(MODELS_DIR, exist_ok=True)\nos.makedirs(PLOTS_DIR, exist_ok=True)\n\n# ---------------- helpers ----------------\ndef find_text_column(df):\n    for c in df.columns:\n        if \"text\" in c.lower():\n            return c\n    for cand in [\"tweet\",\"content\",\"body\"]:\n        for c in df.columns:\n            if cand in c.lower():\n                return c\n    return df.columns[0]\n\ndef safe_read_csv(path):\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"File not found: {path}\")\n    return pd.read_csv(path)\n\n# ---------------- mixed precision ----------------\nif USE_MIXED_PRECISION:\n    try:\n        from tensorflow.keras import mixed_precision\n        mixed_precision.set_global_policy('mixed_float16')\n        print(\"Mixed precision ENABLED.\")\n    except Exception as e:\n        print(\"Mixed precision unavailable, continuing in float32. Error:\", e)\n        USE_MIXED_PRECISION = False\n\n# ---------------- GPU memory growth (help avoid full allocation) ----------------\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for g in gpus:\n            tf.config.experimental.set_memory_growth(g, True)\n        print(\"Enabled GPU memory growth for\", len(gpus), \"GPU(s).\")\n    except Exception as e:\n        print(\"Could not set memory growth:\", e)\n\n# ---------------- load CSVs & preprocess ----------------\nprint(\"Loading CSVs...\")\ndf_mis = safe_read_csv(MISINFO_CSV)\ndf_non = safe_read_csv(NONMISINFO_CSV)\ndf_test = safe_read_csv(TEST_CSV)\n\ntext_col_mis = find_text_column(df_mis)\ntext_col_non = find_text_column(df_non)\ntext_col_test = find_text_column(df_test)\nprint(\"Detected text columns:\", text_col_mis, text_col_non, text_col_test)\n\n# keep only text and label\ndf_mis_small = df_mis[[text_col_mis]].rename(columns={text_col_mis: \"text\"}).copy()\ndf_mis_small[\"label\"] = \"misinfo\"\ndf_non_small = df_non[[text_col_non]].rename(columns={text_col_non: \"text\"}).copy()\ndf_non_small[\"label\"] = \"nonmisinfo\"\n\n# drop empty\ndf_mis_small['text'] = df_mis_small['text'].astype(str).str.strip()\ndf_non_small['text'] = df_non_small['text'].astype(str).str.strip()\ndf_mis_small = df_mis_small[df_mis_small['text'] != \"\"].reset_index(drop=True)\ndf_non_small = df_non_small[df_non_small['text'] != \"\"].reset_index(drop=True)\n\n# downsample non to match mis count (balanced)\nn_mis = len(df_mis_small)\nn_non = len(df_non_small)\nprint(f\"Counts before balancing -> misinfo: {n_mis}, nonmisinfo: {n_non}\")\nif n_mis == 0:\n    raise ValueError(\"No misinfo rows found\")\nif n_non >= n_mis:\n    df_non_down = df_non_small.sample(n=n_mis, random_state=SEED).reset_index(drop=True)\nelse:\n    df_non_down = df_non_small.sample(n=n_mis, replace=True, random_state=SEED).reset_index(drop=True)\n\ntrain_combined = pd.concat([df_mis_small, df_non_down], ignore_index=True)\ntrain_combined = train_combined.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\nprint(\"Combined balanced train shape:\", train_combined.shape)\nprint(train_combined['label'].value_counts())\n\n# prepare test (ensure id)\nif 'id' not in df_test.columns:\n    df_test = df_test.reset_index().rename(columns={'index':'id'})\nelse:\n    df_test['id'] = df_test['id'].ffill().bfill()\ndf_test_small = df_test[['id', text_col_test]].rename(columns={text_col_test:\"text\"}).copy()\ndf_test_small['text'] = df_test_small['text'].astype(str).str.strip().replace(\"\", \"NA\")\n\n# save interim csvs\ntrain_combined[['text','label']].to_csv(OUTPUT_TRAIN_INTERIM, index=False)\ndf_test_small[['id','text']].to_csv(OUTPUT_TEST_INTERIM, index=False)\nprint(\"Saved interim files:\", OUTPUT_TRAIN_INTERIM, OUTPUT_TEST_INTERIM)\n\n# label mapping\nlabel_to_int = {\"misinfo\": 1, \"nonmisinfo\": 0}\ntrain_combined['label_int'] = train_combined['label'].map(label_to_int).astype(int)\n\n# stratified split 80:20\ntrain_df, val_df = train_test_split(train_combined, test_size=0.2, random_state=SEED, stratify=train_combined['label_int'])\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\nprint(\"Train, Val, Test sizes:\", len(train_df), len(val_df), len(df_test_small))\n\n# ---------------- tokenizer & encoder ----------------\nprint(\"Loading tokenizer & encoder:\", BERT_NAME)\ntokenizer = AutoTokenizer.from_pretrained(BERT_NAME)\nencoder = TFRobertaModel.from_pretrained(BERT_NAME)\n\n# Freeze encoder if requested (massive memory saver)\nif FREEZE_ENCODER:\n    encoder.trainable = False\n    print(\"Encoder frozen (FREEZE_ENCODER=True). Only heads will be trained.\")\n    if UNFREEZE_LAST_N > 0:\n        # try to unfreeze last N transformer encoder layers (if desired)\n        try:\n            # transformer encoder layers are usually named like \"roberta/encoder/layer_{i}\"\n            encoder.trainable = True\n            for var in encoder.variables:\n                var.trainable = False\n            # find layer variables and set last UNFREEZE_LAST_N layers to trainable\n            layer_vars = [v for v in encoder.variables if \"encoder/layer\" in v.name]\n            # infer layer count\n            layer_names = sorted({v.name.split(\"/encoder/layer_._\")[-1].split(\"/\")[0] for v in layer_vars})\n            # This is best-effort; if it fails we simply keep encoder frozen\n            num_layers = len(layer_names)\n            if UNFREEZE_LAST_N >= num_layers:\n                UNFREEZE_LAST_N = num_layers\n            # set trainable for last N layers\n            for v in encoder.variables:\n                for i in range(num_layers - UNFREEZE_LAST_N, num_layers):\n                    if f\"encoder/layer_._{i}\" in v.name:\n                        v._trainable = True\n            print(f\"Unfroze last {UNFREEZE_LAST_N} encoder layers (best-effort).\")\n        except Exception as e:\n            print(\"Could not partially unfreeze encoder, keeping it frozen. Error:\", e)\n            encoder.trainable = False\n\n# ---------------- tokenization helper (batched & memory-friendly) ----------------\ndef tokenize_texts(texts, desc=\"tokenize\", batch_size=256):\n    all_input_ids = []\n    all_attention = []\n    n = len(texts)\n    steps = math.ceil(n / batch_size)\n    for i in tqdm(range(0, n, batch_size), total=steps, desc=desc, unit=\"batch\"):\n        batch = texts[i:i+batch_size]\n        toks = tokenizer(batch, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"tf\")\n        all_input_ids.append(toks[\"input_ids\"])\n        all_attention.append(toks[\"attention_mask\"])\n    input_ids = tf.concat(all_input_ids, axis=0)\n    attention_mask = tf.concat(all_attention, axis=0)\n    return input_ids, attention_mask\n\nprint(\"Tokenizing train/val/test (shows progress)...\")\ntrain_input_ids, train_attention = tokenize_texts(train_df['text'].astype(str).tolist(), desc=\"Tokenize train\", batch_size=128)\nval_input_ids, val_attention = tokenize_texts(val_df['text'].astype(str).tolist(), desc=\"Tokenize val\", batch_size=128)\ntest_input_ids, test_attention = tokenize_texts(df_test_small['text'].astype(str).tolist(), desc=\"Tokenize test\", batch_size=128)\n\ntrain_labels = train_df['label_int'].values.astype(np.int32)\nval_labels = val_df['label_int'].values.astype(np.int32)\n\n# build tf.data datasets (shuffle only train)\ntrain_ds = tf.data.Dataset.from_tensor_slices(((train_input_ids, train_attention), train_labels))\ntrain_ds = train_ds.shuffle(buffer_size=2048, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nval_ds = tf.data.Dataset.from_tensor_slices(((val_input_ids, val_attention), val_labels)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_ds = tf.data.Dataset.from_tensor_slices(((test_input_ids, test_attention), np.zeros((test_input_ids.shape[0],), dtype=np.int32))).batch(BATCH_SIZE)\n\n# ---------------- Model components ----------------\ndef mean_pool(last_hidden, mask):\n    last_hidden_dtype = last_hidden.dtype\n    mask = tf.cast(tf.expand_dims(mask, -1), last_hidden_dtype)\n    summed = tf.reduce_sum(last_hidden * mask, axis=1)\n    counts = tf.reduce_sum(mask, axis=1) + tf.constant(1e-10, dtype=last_hidden_dtype)\n    return summed / counts\n\n# projection head: includes dropout so calling it twice yields two stochastic views without re-running encoder.\ndef build_projection_head(proj_dim=PROJ_DIM, dropout_rate=0.1):\n    inp = tf.keras.Input(shape=(encoder.config.hidden_size,), dtype=tf.float32)\n    x = tf.keras.layers.Dense(encoder.config.hidden_size, activation=\"gelu\", dtype=\"float32\")(inp)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.Dense(proj_dim, activation=None, dtype=\"float32\")(x)\n    x = tf.keras.layers.LayerNormalization(dtype=\"float32\")(x)\n    out = tf.keras.layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1), dtype=\"float32\")(x)\n    return tf.keras.Model(inp, out, name=\"proj_head\")\n\ndef build_classifier_head():\n    inp = tf.keras.Input(shape=(encoder.config.hidden_size,), dtype=tf.float32)\n    x = tf.keras.layers.Dense(256, activation=\"gelu\", dtype=\"float32\")(inp)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\")(x)\n    return tf.keras.Model(inp, out, name=\"clf_head\")\n\nproj_head = build_projection_head()\nclf_head = build_classifier_head()\n\n# collect trainable variables: depending on freeze\ntrainable_vars = proj_head.trainable_variables + clf_head.trainable_variables\nif not FREEZE_ENCODER:\n    trainable_vars = list(encoder.trainable_variables) + trainable_vars\n\n# optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n\nbce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n\n# ---------------- supervised contrastive loss (vectorized, dtype-safe) ----------------\n@tf.function\ndef supervised_contrastive_loss(projections, labels, temperature=TEMPERATURE):\n    projections = tf.cast(projections, tf.float32)   # (2N, d)\n    labels = tf.cast(labels, tf.int32)\n\n    logits = tf.matmul(projections, projections, transpose_b=True) / tf.cast(temperature, tf.float32)  # (2N,2N)\n    n = tf.shape(logits)[0]\n    diag_mask = tf.eye(n, dtype=tf.bool)\n\n    very_neg = tf.constant(-1e12, dtype=logits.dtype)\n    logits_masked = tf.where(diag_mask, tf.fill(tf.shape(logits), very_neg), logits)\n\n    labels_eq = tf.cast(tf.equal(tf.expand_dims(labels, 1), tf.expand_dims(labels, 0)), logits.dtype)\n    labels_eq = labels_eq - tf.cast(tf.eye(n), logits.dtype)\n\n    exp_logits = tf.exp(logits_masked)\n\n    exp_pos = tf.reduce_sum(exp_logits * labels_eq, axis=1)\n    exp_all = tf.reduce_sum(exp_logits, axis=1)\n\n    pos_counts = tf.reduce_sum(labels_eq, axis=1)\n\n    eps = tf.constant(1e-12, dtype=logits.dtype)\n    ratio = tf.where(pos_counts > 0, exp_pos / (exp_all + eps), tf.zeros_like(exp_pos))\n    loss_per_anchor = -tf.math.log(ratio + eps)\n\n    valid = tf.cast(pos_counts > 0, logits.dtype)\n    loss = tf.reduce_sum(loss_per_anchor * valid) / (tf.reduce_sum(valid) + eps)\n\n    return tf.cast(loss, tf.float32)\n\n# ---------------- training step (single encoder forward) ----------------\n@tf.function\ndef train_step(input_ids, attention_mask, labels):\n    labels = tf.cast(labels, tf.float32)\n    with tf.GradientTape() as tape:\n        # single encoder forward (memory saver)\n        enc_out = encoder(input_ids, attention_mask=attention_mask, training=False if FREEZE_ENCODER else True).last_hidden_state\n        pool = mean_pool(enc_out, attention_mask)  # (batch, hidden)\n\n        pool_f32 = tf.cast(pool, tf.float32)\n\n        # get two stochastic projections by calling proj_head twice with training=True (dropout in proj head)\n        z1 = proj_head(pool_f32, training=True)\n        z2 = proj_head(pool_f32, training=True)\n        z = tf.concat([z1, z2], axis=0)\n        labels_twice = tf.concat([labels, labels], axis=0)\n\n        c_loss = supervised_contrastive_loss(z, tf.cast(labels_twice, tf.int32))\n\n        preds = clf_head(pool_f32, training=True)\n        cls_loss = tf.reduce_mean(bce(tf.reshape(labels, (-1,1)), preds))\n\n        total_loss = ALPHA * c_loss + BETA * cls_loss\n\n    grads = tape.gradient(total_loss, trainable_vars)\n    # gradient clipping (avoid large steps)\n    grads, _ = tf.clip_by_global_norm(grads, 1.0)\n    optimizer.apply_gradients(zip(grads, trainable_vars))\n\n    return tf.cast(total_loss, tf.float32), tf.cast(c_loss, tf.float32), tf.cast(cls_loss, tf.float32)\n\n# ---------------- validation ----------------\ndef evaluate_on_val():\n    preds_all = []\n    y_all = []\n    for (batch_ids, batch_mask), batch_labels in val_ds:\n        out = encoder(batch_ids, attention_mask=batch_mask, training=False).last_hidden_state\n        pool = mean_pool(out, batch_mask)\n        preds = clf_head(tf.cast(pool, tf.float32), training=False).numpy().ravel()\n        preds_all.extend(preds.tolist())\n        y_all.extend(batch_labels.numpy().tolist())\n    preds_bin = (np.array(preds_all) >= 0.5).astype(int)\n    y_all = np.array(y_all).astype(int)\n    prec = precision_score(y_all, preds_bin, zero_division=0)\n    rec = recall_score(y_all, preds_bin, zero_division=0)\n    f1 = f1_score(y_all, preds_bin, zero_division=0)\n    return prec, rec, f1, preds_all, y_all\n\n# ---------------- training loop ----------------\nbest_val_f1 = -1.0\nepochs_since_improve = 0\nepoch_list, total_losses, c_losses, cls_losses = [], [], [], []\nval_precisions, val_recalls, val_f1s = [], [], []\nlr_history = []\n\nnum_steps = math.ceil(len(train_df) / BATCH_SIZE)\nprint(\"\\nStarting contrastive training for up to\", EPOCHS, \"epochs\")\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_list.append(epoch)\n    batch_total_losses, batch_c_losses, batch_cls_losses = [], [], []\n    with tqdm(total=num_steps, desc=f\"Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n        for (batch_ids, batch_mask), batch_labels in train_ds:\n            try:\n                t_loss, t_c_loss, t_cls_loss = train_step(batch_ids, batch_mask, batch_labels)\n            except tf.errors.ResourceExhaustedError as e:\n                print(\"ResourceExhaustedError during train_step; consider reducing BATCH_SIZE or MAX_LEN.\")\n                raise e\n            batch_total_losses.append(float(t_loss.numpy()))\n            batch_c_losses.append(float(t_c_loss.numpy()))\n            batch_cls_losses.append(float(t_cls_loss.numpy()))\n            pbar.set_postfix({\n                \"tot\": f\"{np.mean(batch_total_losses):.4f}\",\n                \"c\": f\"{np.mean(batch_c_losses):.4f}\",\n                \"cls\": f\"{np.mean(batch_cls_losses):.4f}\"\n            })\n            pbar.update(1)\n\n    avg_total = np.mean(batch_total_losses)\n    avg_c = np.mean(batch_c_losses)\n    avg_cls = np.mean(batch_cls_losses)\n    total_losses.append(avg_total); c_losses.append(avg_c); cls_losses.append(avg_cls)\n\n    # evaluate\n    prec, rec, f1, _, _ = evaluate_on_val()\n    val_precisions.append(prec); val_recalls.append(rec); val_f1s.append(f1)\n\n    # LR history\n    try:\n        current_lr = float(optimizer.lr.numpy())\n    except Exception:\n        current_lr = float(LR)\n    lr_history.append(current_lr)\n\n    print(f\"\\nEpoch {epoch} summary: tot_loss={avg_total:.4f} contrastive={avg_c:.4f} cls={avg_cls:.4f}\")\n    print(f\" Val -> Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f} | LR: {current_lr:.2e}\")\n\n    # save best\n    if f1 > best_val_f1 + 1e-6:\n        best_val_f1 = f1\n        epochs_since_improve = 0\n        try:\n            proj_head.save_weights(os.path.join(MODELS_DIR, \"best_proj_weights.h5\"))\n            clf_head.save_weights(os.path.join(MODELS_DIR, \"best_clf_weights.h5\"))\n            if not FREEZE_ENCODER:\n                encoder.save_weights(os.path.join(MODELS_DIR, \"best_encoder_weights.h5\"))\n            print(\"  >> New best val F1, head weights saved.\")\n        except Exception as e:\n            print(\"  >> Warning: saving weights failed:\", e)\n    else:\n        epochs_since_improve += 1\n        print(f\"  >> No improvement for {epochs_since_improve} epoch(s).\")\n\n    # LR reduce (simple)\n    if epochs_since_improve > 0 and epochs_since_improve % lr_reduce_patience == 0:\n        new_lr = current_lr * lr_reduce_factor\n        try:\n            tf.keras.backend.set_value(optimizer.lr, new_lr)\n            print(f\"  >> Reduced LR to {new_lr:.2e}\")\n        except Exception:\n            print(\"  >> Could not reduce LR via backend set_value.\")\n\n    if epochs_since_improve >= early_stop_patience:\n        print(f\"\\nEarly stopping: no improvement for {epochs_since_improve} epochs.\")\n        break\n\n# restore best weights if exist\nbest_proj = os.path.join(MODELS_DIR, \"best_proj_weights.h5\")\nbest_clf = os.path.join(MODELS_DIR, \"best_clf_weights.h5\")\nbest_encoder = os.path.join(MODELS_DIR, \"best_encoder_weights.h5\")\n\nif os.path.exists(best_proj) and os.path.exists(best_clf):\n    try:\n        print(\"Restoring best head weights...\")\n        proj_head.load_weights(best_proj)\n        clf_head.load_weights(best_clf)\n    except Exception as e:\n        print(\"Warning: could not restore head weights:\", e)\nif not FREEZE_ENCODER and os.path.exists(best_encoder):\n    try:\n        encoder.load_weights(best_encoder)\n    except Exception as e:\n        print(\"Warning: could not restore encoder weights:\", e)\n\n# ---------------- plotting (optional) ----------------\nif epoch_list:\n    plt.figure(figsize=(8,5))\n    plt.plot(epoch_list, total_losses, label=\"total_loss\")\n    plt.plot(epoch_list, c_losses, label=\"contrastive_loss\")\n    plt.plot(epoch_list, cls_losses, label=\"classifier_loss\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training losses\"); plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR, \"train_losses.png\"), dpi=200); plt.close()\n\n    plt.figure(figsize=(8,5))\n    plt.plot(epoch_list, val_precisions, marker='o', label=\"precision\")\n    plt.plot(epoch_list, val_recalls, marker='o', label=\"recall\")\n    plt.plot(epoch_list, val_f1s, marker='o', label=\"f1\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(\"Validation metrics\"); plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR, \"val_metrics.png\"), dpi=200); plt.close()\n\n    plt.figure(figsize=(8,5))\n    plt.plot(epoch_list, lr_history, marker='o', label=\"lr\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Learning rate\"); plt.title(\"LR schedule\"); plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR, \"lr_schedule.png\"), dpi=200); plt.close()\n\nprint(\"Saved plots to\", PLOTS_DIR)\n\n# ---------------- final predictions on test ----------------\nprint(\"Running final predictions on test set (with progress)...\")\ntest_preds = []\ntest_steps = math.ceil(len(df_test_small) / BATCH_SIZE)\nfor (batch_ids, batch_mask), _ in tqdm(test_ds, total=test_steps, desc=\"Test predict\", unit=\"batch\"):\n    out = encoder(batch_ids, attention_mask=batch_mask, training=False).last_hidden_state\n    pool = mean_pool(out, batch_mask)\n    preds = clf_head(tf.cast(pool, tf.float32), training=False).numpy().ravel()\n    test_preds.extend(preds.tolist())\n\ntest_preds = np.array(test_preds)\ntest_pred_bins = (test_preds >= 0.5).astype(int)\nlabel_map_back = {1: \"misinfo\", 0: \"nonmisinfo\"}\npred_label_str = [label_map_back[int(x)] for x in test_pred_bins]\n\nout_df = pd.DataFrame({\"id\": df_test_small['id'].astype(int), \"label\": pred_label_str})\nout_df.to_csv(PRED_CSV, index=False)\nprint(\"Saved predictions to\", PRED_CSV, \"(columns: id,label)\")\n\n# save final head weights (encoder saved only if not frozen)\ntry:\n    proj_head.save_weights(os.path.join(MODELS_DIR, \"proj_head_weights.h5\"))\n    clf_head.save_weights(os.path.join(MODELS_DIR, \"clf_head_weights.h5\"))\n    if not FREEZE_ENCODER:\n        encoder.save_pretrained(os.path.join(MODELS_DIR, \"encoder_saved\"))\nexcept Exception as e:\n    print(\"Warning: saving model artifacts failed:\", e)\n\nprint(\"Saved model artifacts to\", MODELS_DIR)\nprint(\"\\nReference (local PDF path):\", PDF_REF_PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-22T11:50:25.695487Z","iopub.execute_input":"2025-11-22T11:50:25.695819Z","iopub.status.idle":"2025-11-22T11:57:14.167893Z","shell.execute_reply.started":"2025-11-22T11:50:25.695792Z","shell.execute_reply":"2025-11-22T11:57:14.167122Z"}},"outputs":[{"name":"stdout","text":"Enabled GPU memory growth for 1 GPU(s).\nLoading CSVs...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/25118239.py:89: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n  return pd.read_csv(path)\n","output_type":"stream"},{"name":"stdout","text":"Detected text columns: text text text\nCounts before balancing -> misinfo: 364, nonmisinfo: 34174\nCombined balanced train shape: (728, 2)\nlabel\nmisinfo       364\nnonmisinfo    364\nName: count, dtype: int64\nSaved interim files: train_final_llm.csv test_final_llm.csv\nTrain, Val, Test sizes: 582 146 2414\nLoading tokenizer & encoder: roberta-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.bias']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Encoder frozen (FREEZE_ENCODER=True). Only heads will be trained.\nTokenizing train/val/test (shows progress)...\n","output_type":"stream"},{"name":"stderr","text":"Tokenize train: 100%|██████████| 5/5 [00:00<00:00, 63.87batch/s]\nTokenize val: 100%|██████████| 2/2 [00:00<00:00, 101.92batch/s]\nTokenize test: 100%|██████████| 19/19 [00:00<00:00, 24.14batch/s]\n","output_type":"stream"},{"name":"stdout","text":"\nStarting contrastive training for up to 50 epochs\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 37/37 [00:24<00:00,  1.51batch/s, tot=1.0119, c=0.3551, cls=0.6568]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 summary: tot_loss=1.0119 contrastive=0.3551 cls=0.6568\n Val -> Precision: 0.6701, Recall: 0.8904, F1: 0.7647 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.6919, c=0.1383, cls=0.5536]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 summary: tot_loss=0.6919 contrastive=0.1383 cls=0.5536\n Val -> Precision: 0.6667, Recall: 0.8219, F1: 0.7362 | LR: 3.00e-04\n  >> No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 37/37 [00:12<00:00,  2.94batch/s, tot=0.5536, c=0.0599, cls=0.4937]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 summary: tot_loss=0.5536 contrastive=0.0599 cls=0.4937\n Val -> Precision: 0.7317, Recall: 0.8219, F1: 0.7742 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.4833, c=0.0499, cls=0.4334]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 summary: tot_loss=0.4833 contrastive=0.0499 cls=0.4334\n Val -> Precision: 0.7111, Recall: 0.8767, F1: 0.7853 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 37/37 [00:12<00:00,  2.94batch/s, tot=0.4307, c=0.0233, cls=0.4074]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 summary: tot_loss=0.4307 contrastive=0.0233 cls=0.4074\n Val -> Precision: 0.7065, Recall: 0.8904, F1: 0.7879 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.3918, c=0.0165, cls=0.3753]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 summary: tot_loss=0.3918 contrastive=0.0165 cls=0.3753\n Val -> Precision: 0.7191, Recall: 0.8767, F1: 0.7901 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.3694, c=0.0081, cls=0.3613]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 summary: tot_loss=0.3694 contrastive=0.0081 cls=0.3613\n Val -> Precision: 0.7973, Recall: 0.8082, F1: 0.8027 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.3576, c=0.0070, cls=0.3506]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 summary: tot_loss=0.3576 contrastive=0.0070 cls=0.3506\n Val -> Precision: 0.7619, Recall: 0.8767, F1: 0.8153 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.3278, c=0.0068, cls=0.3210]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9 summary: tot_loss=0.3278 contrastive=0.0068 cls=0.3210\n Val -> Precision: 0.7975, Recall: 0.8630, F1: 0.8289 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 37/37 [00:12<00:00,  2.94batch/s, tot=0.3201, c=0.0057, cls=0.3144]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10 summary: tot_loss=0.3201 contrastive=0.0057 cls=0.3144\n Val -> Precision: 0.7683, Recall: 0.8630, F1: 0.8129 | LR: 3.00e-04\n  >> No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.3102, c=0.0049, cls=0.3053]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11 summary: tot_loss=0.3102 contrastive=0.0049 cls=0.3053\n Val -> Precision: 0.7831, Recall: 0.8904, F1: 0.8333 | LR: 3.00e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 37/37 [00:12<00:00,  2.94batch/s, tot=0.2884, c=0.0022, cls=0.2862]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12 summary: tot_loss=0.2884 contrastive=0.0022 cls=0.2862\n Val -> Precision: 0.7738, Recall: 0.8904, F1: 0.8280 | LR: 3.00e-04\n  >> No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2893, c=0.0032, cls=0.2862]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13 summary: tot_loss=0.2893 contrastive=0.0032 cls=0.2862\n Val -> Precision: 0.7647, Recall: 0.8904, F1: 0.8228 | LR: 3.00e-04\n  >> No improvement for 2 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 37/37 [00:12<00:00,  2.94batch/s, tot=0.2737, c=0.0053, cls=0.2684]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14 summary: tot_loss=0.2737 contrastive=0.0053 cls=0.2684\n Val -> Precision: 0.7838, Recall: 0.7945, F1: 0.7891 | LR: 3.00e-04\n  >> No improvement for 3 epoch(s).\n  >> Reduced LR to 1.50e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2636, c=0.0031, cls=0.2605]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15 summary: tot_loss=0.2636 contrastive=0.0031 cls=0.2605\n Val -> Precision: 0.7927, Recall: 0.8904, F1: 0.8387 | LR: 1.50e-04\n  >> New best val F1, head weights saved.\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2640, c=0.0019, cls=0.2621]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16 summary: tot_loss=0.2640 contrastive=0.0019 cls=0.2621\n Val -> Precision: 0.7763, Recall: 0.8082, F1: 0.7919 | LR: 1.50e-04\n  >> No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2449, c=0.0015, cls=0.2434]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17 summary: tot_loss=0.2449 contrastive=0.0015 cls=0.2434\n Val -> Precision: 0.7805, Recall: 0.8767, F1: 0.8258 | LR: 1.50e-04\n  >> No improvement for 2 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2531, c=0.0043, cls=0.2488]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18 summary: tot_loss=0.2531 contrastive=0.0043 cls=0.2488\n Val -> Precision: 0.7945, Recall: 0.7945, F1: 0.7945 | LR: 1.50e-04\n  >> No improvement for 3 epoch(s).\n  >> Reduced LR to 7.50e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2463, c=0.0018, cls=0.2445]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19 summary: tot_loss=0.2463 contrastive=0.0018 cls=0.2445\n Val -> Precision: 0.7831, Recall: 0.8904, F1: 0.8333 | LR: 7.50e-05\n  >> No improvement for 4 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2434, c=0.0018, cls=0.2416]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20 summary: tot_loss=0.2434 contrastive=0.0018 cls=0.2416\n Val -> Precision: 0.7805, Recall: 0.8767, F1: 0.8258 | LR: 7.50e-05\n  >> No improvement for 5 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 37/37 [00:12<00:00,  2.95batch/s, tot=0.2387, c=0.0011, cls=0.2376]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21 summary: tot_loss=0.2387 contrastive=0.0011 cls=0.2376\n Val -> Precision: 0.7805, Recall: 0.8767, F1: 0.8258 | LR: 7.50e-05\n  >> No improvement for 6 epoch(s).\n  >> Reduced LR to 3.75e-05\n\nEarly stopping: no improvement for 6 epochs.\nRestoring best head weights...\nSaved plots to plots\nRunning final predictions on test set (with progress)...\n","output_type":"stream"},{"name":"stderr","text":"Test predict: 100%|██████████| 151/151 [00:54<00:00,  2.75batch/s]","output_type":"stream"},{"name":"stdout","text":"Saved predictions to predictions.csv (columns: id,label)\nSaved model artifacts to models\n\nReference (local PDF path): /mnt/data/Fake_News_Detection_in_Social_Media_Hybrid_Deep_Le.pdf\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}